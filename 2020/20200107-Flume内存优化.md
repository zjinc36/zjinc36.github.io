#   Flume内存优化
+ date: 2020-01-07 10:24:50
+ description: Flume内存优化
+ categories:
  - BigData
+ tags:
  - Flume
---
#   问题描述：如果启动消费Flume抛出如下异常
```
ERROR hdfs.HDFSEventSink: process failed java.lang.OutOfMemoryError: GC overhead limit exceeded
```

#   解决方案步骤
1.  在hadoop102服务器的/opt/module/flume/conf/flume-env.sh文件中增加如下配置
```
export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote"
```

2.  同步配置到hadoop103、hadoop104服务器
```
$ xsync flume-env.sh
```

#   Flume内存参数设置及优化
JVM heap一般设置为4G或更高，部署在单独的服务器上（4核8线程16G内存）
-Xmx与-Xms最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。
