<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><title>人工智能数学基础之数学分析 =&gt; 梯度下降算法 | 想了20分钟的博客名</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">人工智能数学基础之数学分析 =&gt; 梯度下降算法</h1><a id="logo" href="/.">想了20分钟的博客名</a><p class="description">世界是唯物辩证的</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">人工智能数学基础之数学分析 =&gt; 梯度下降算法</h1><div class="post-meta"><a href="/2019/08/18/2019-20190817-人工智能数学基础之数学分析/#comments" class="comment-count"></a><p><span class="date">Aug 18, 2019</span><span><a href="/categories/人工智能/" class="category">人工智能</a></span></p></div><div class="post-content"><h1 id="映射与函数"><a href="#映射与函数" class="headerlink" title="映射与函数"></a>映射与函数</h1><p><img src="/images/2020/08/20200817093610.png" alt></p>
<h1 id="极限"><a href="#极限" class="headerlink" title="极限"></a>极限</h1><p><img src="/images/2020/08/20200817090633.png" alt></p>
<h1 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h1><ul>
<li>导数是曲线的斜率，是曲线变化</li>
<li>快慢的反应；</li>
<li>可导一定连续，连续不一定可导</li>
</ul>
<h1 id="极值如何求解"><a href="#极值如何求解" class="headerlink" title="极值如何求解"></a>极值如何求解</h1><p>导数为0的位置<br><img src="/images/2020/08/20200817095813.png" alt></p>
<h1 id="常用函数求导公式"><a href="#常用函数求导公式" class="headerlink" title="常用函数求导公式"></a>常用函数求导公式</h1><p><img src="/images/2020/08/20200817095909.png" alt></p>
<h1 id="求sigmoid函数导数"><a href="#求sigmoid函数导数" class="headerlink" title="求sigmoid函数导数"></a>求sigmoid函数导数</h1><p><img src="/images/2020/08/20200817100429.png" alt><br><img src="/images/2020/08/20200817100409.png" alt><br>导数为<br><img src="/images/2020/08/20200817100340.png" alt></p>
<p>说明</p>
<ul>
<li>导数不会求着求着就没了</li>
<li>它是非线性的,非线性可以将数据映射到一个其他维度的空间上去</li>
</ul>
<h1 id="泰勒展开式"><a href="#泰勒展开式" class="headerlink" title="泰勒展开式"></a>泰勒展开式</h1><p><img src="/images/2020/08/20200817111955.png" alt></p>
<p>常用函数的泰勒展开</p>
<ul>
<li>在某邻域内，存在一阶近似、二阶近似、…. 逼近非线性函数求解</li>
</ul>
<p><img src="/images/2020/08/20200817112226.png" alt></p>
<h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/c7e642877b0e" target="_blank" rel="noopener">深入浅出–梯度下降法及其实现</a></p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>用最通俗的语言来介绍下：</p>
<p>假设你站在华山之巅，你现在想以最快速度下山，那你肯定是找一条最陡峭的路走。你环顾四周，找到了一条路线，恩，这个方向是最陡的。于是你就出发了，走了一会发现，这个方向不是最陡的路了。你就停下来，换了个最陡的方向，继续往下走。重复这个步骤，你最终到达了山脚下。</p>
<p>那么，你从山顶到山脚的整个下山的过程，就是梯度下降。</p>
<h2 id="困难点"><a href="#困难点" class="headerlink" title="困难点"></a>困难点</h2><ul>
<li>可以假设这座山最陡峭的地方是无法通过肉眼立马观察出来的，而是需要一个复杂的工具来测量，同时，你此时正好拥有测量出最陡峭方向的能力。所以，你每走一段距离，都需要一段时间来测量所在位置最陡峭的方向，这是比较耗时的。那么为了在太阳下山之前到达山底，就要尽可能的减少测量方向的次数。</li>
<li>这是一个两难的选择，如果测量的频繁，可以保证下山的方向是绝对正确的，但又非常耗时，如果测量的过少，又有偏离轨道的风险。</li>
<li>所以需要找到一个合适的测量方向的频率，来确保下山的方向不错误，同时又不至于耗时太多！</li>
</ul>
<h2 id="什么是梯度"><a href="#什么是梯度" class="headerlink" title="什么是梯度"></a>什么是梯度</h2><h3 id="偏微分-偏导"><a href="#偏微分-偏导" class="headerlink" title="偏微分(偏导)"></a>偏微分(偏导)</h3><p>看待微分的意义，可以有不同的角度，最常用的两种是：</p>
<p>函数图像中，某点的切线的斜率</p>
<ul>
<li>函数的变化率</li>
<li>几个微分的例子：<br><img src="/images/2020/08/20200817113733.png" alt></li>
</ul>
<p>上面的例子都是单变量的微分，当一个函数有多个变量的时候，就有了多变量的微分，即分别对每个变量进行求微分<br><img src="/images/2020/08/20200817113757.png" alt></p>
<h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>梯度实际上就是多变量微分的一般化。<br><img src="/images/2020/08/20200817113942.png" alt><br>我们可以看到，梯度就是分别对每个变量进行微分，然后用逗号分割开，梯度是用&lt;&gt;包括起来，说明<code>梯度其实一个向量</code>。如果是一个二维向量(两个力的方向),显然<code>线性相加和(合力)是最大的,称之为方向导数</code><br><img src="/images/2020/08/20200817114149.png" alt></p>
<p>梯度是微积分中一个很重要的概念，之前提到过梯度的意义</p>
<ul>
<li>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率</li>
<li>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向</li>
</ul>
<p>这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的方向一直走，就能走到局部的最低点！</p>
<h2 id="梯度下降算法的数学解释"><a href="#梯度下降算法的数学解释" class="headerlink" title="梯度下降算法的数学解释"></a>梯度下降算法的数学解释</h2><h3 id="数学公式说明"><a href="#数学公式说明" class="headerlink" title="数学公式说明"></a>数学公式说明</h3><p>上面我们花了大量的篇幅介绍梯度下降算法的基本思想和场景假设，以及梯度的概念和思想。下面我们就开始从数学上解释梯度下降算法的计算过程和思想！<br><img src="/images/2020/08/20200817114732.png" alt><br>此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，也就是山底。首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！<br><img src="/images/2020/08/20200817114853.png" alt></p>
<p>几点说明</p>
<h3 id="α是什么含义？"><a href="#α是什么含义？" class="headerlink" title="α是什么含义？"></a>α是什么含义？</h3><p>α在梯度下降算法中被称作为学习率或者步长，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！<br><img src="/images/2020/08/20200817115052.png" alt></p>
<h3 id="为什么要梯度要乘以一个负号？"><a href="#为什么要梯度要乘以一个负号？" class="headerlink" title="为什么要梯度要乘以一个负号？"></a>为什么要梯度要乘以一个负号？</h3><p>梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号</p>
<h2 id="梯度下降算法的实例"><a href="#梯度下降算法的实例" class="headerlink" title="梯度下降算法的实例"></a>梯度下降算法的实例</h2><p>我们已经基本了解了梯度下降算法的计算过程，那么我们就来看几个梯度下降算法的小实例，首先从单变量的函数开始</p>
<h3 id="单变量函数的梯度下降"><a href="#单变量函数的梯度下降" class="headerlink" title="单变量函数的梯度下降"></a>单变量函数的梯度下降</h3><p>我们假设有一个单变量的函数<br><img src="/images/2020/08/20200817115231.png" alt></p>
<p>函数的微分<br><img src="/images/2020/08/20200817115245.png" alt></p>
<p>初始化，起点为<br><img src="/images/2020/08/20200817115308.png" alt></p>
<p>学习率为<br><img src="/images/2020/08/20200817115318.png" alt></p>
<p>根据梯度下降的计算公式<br><img src="/images/2020/08/20200817115330.png" alt></p>
<p>我们开始进行梯度下降的迭代计算过程：<br><img src="/images/2020/08/20200817115344.png" alt></p>
<p>如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底<br><img src="/images/2020/08/20200817115417.png" alt></p>
<h3 id="多变量函数的梯度下降"><a href="#多变量函数的梯度下降" class="headerlink" title="多变量函数的梯度下降"></a>多变量函数的梯度下降</h3><p>我们假设有一个目标函数<br><img src="/images/2020/08/20200817115633.png" alt></p>
<p>现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下来，我们会从梯度下降算法开始一步步计算到这个最小值！<br>我们假设初始的起点为：<br><img src="/images/2020/08/20200817115654.png" alt></p>
<p>初始的学习率为<br><img src="/images/2020/08/20200817115705.png" alt></p>
<p>函数的梯度为<br><img src="/images/2020/08/20200817115716.png" alt></p>
<p>进行多次迭代<br><img src="/images/2020/08/20200817115738.png" alt></p>
<p>我们发现，已经基本靠近函数的最小值点<br><img src="/images/2020/08/20200817115756.png" alt></p>
<h2 id="梯度下降算法的实现"><a href="#梯度下降算法的实现" class="headerlink" title="梯度下降算法的实现"></a>梯度下降算法的实现</h2><h3 id="简单说明最小二乘法"><a href="#简单说明最小二乘法" class="headerlink" title="简单说明最小二乘法"></a>简单说明最小二乘法</h3><p>某次实验得到了四个数据点 (x ,y):(1,6)、(2,5)、(3,7)、(4,10)（下图红色的点）。<br><img src="/images/2020/08/20200817121936.png" alt><br>我们希望找出一条和这四个点最匹配的直线<br><img src="/images/2020/08/20200817121859.png" alt></p>
<p>最小二乘法采用的方法是尽量使得等号两边的平方差最小，也就是找出这个函数的最小值：<br><img src="/images/2020/08/20200817122053.png" alt><br>最小值可以通过对上述函数分别求偏导数,然后使他们等于零得到。<br><img src="/images/2020/08/20200817122217.png" alt><br>如此就得到了一个只有两个未知数的方程组，很容易就可以解出：<br><img src="/images/2020/08/20200817122233.png" alt><br>也就是说直线<code>y = 3.5 + 1.4x</code>是最佳的。</p>
<h3 id="梯度下降算法的实现-1"><a href="#梯度下降算法的实现-1" class="headerlink" title="梯度下降算法的实现"></a>梯度下降算法的实现</h3><p>下面我们将用python实现一个简单的梯度下降算法。场景是一个简单的<a href="https://en.wikipedia.org/wiki/Linear_regression" target="_blank" rel="noopener">线性回归</a>的例子：假设现在我们有一系列的点，如下图所示<br><img src="/images/2020/08/20200817121052.png" alt></p>
<p>我们将用梯度下降法来拟合出这条直线！<br>首先，我们需要定义一个代价函数，在此我们选用<a href="https://en.wikipedia.org/wiki/Least_squares" target="_blank" rel="noopener">均方误差代价函数(最小二乘法)</a><br><img src="/images/2020/08/20200817121111.png" alt></p>
<p>此公式中</p>
<ul>
<li>m是数据集中点的个数</li>
<li>½是一个常量，这样是为了在求梯度的时候，二次方乘下来就和这里的½抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响</li>
<li>y 是数据集中每个点的真实y坐标的值</li>
<li>h 是我们的预测函数，根据每一个输入x，根据Θ 计算得到预测的y值，即<br><img src="/images/2020/08/20200817122430.png" alt><br>我们可以根据代价函数看到，代价函数中的变量有两个，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量进行微分<br><img src="/images/2020/08/20200817122452.png" alt></li>
</ul>
<p>明确了代价函数和梯度，以及预测的函数形式。我们就可以开始编写代码了。但在这之前，需要说明一点，就是为了方便代码的编写，我们会将所有的公式都转换为矩阵的形式，python中计算矩阵是非常方便的，同时代码也会变得非常的简洁。</p>
<p>为了转换为矩阵的计算，我们观察到预测函数的形式<br><img src="/images/2020/08/20200817122512.png" alt></p>
<p>我们有两个变量，为了对这个公式进行矩阵化，我们可以给每一个点x增加一维，这一维的值固定为1，这一维将会乘到Θ0上。这样就方便我们统一矩阵化的计算<br><img src="/images/2020/08/20200817122523.png" alt></p>
<p>然后我们将代价函数和梯度转化为矩阵向量相乘的形式<br><img src="/images/2020/08/20200817122535.png" alt></p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>首先，我们需要定义数据集和学习率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of the points dataset.</span></span><br><span class="line">m = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Points x-coordinate and dummy value (x0, x1).</span></span><br><span class="line">X0 = np.ones((m, <span class="number">1</span>))</span><br><span class="line">X1 = np.arange(<span class="number">1</span>, m+<span class="number">1</span>).reshape(m, <span class="number">1</span>)</span><br><span class="line">X = np.hstack((X0, X1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Points y-coordinate</span></span><br><span class="line">y = np.array([</span><br><span class="line">    <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">11</span>, <span class="number">8</span>, <span class="number">12</span>,</span><br><span class="line">    <span class="number">11</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">21</span></span><br><span class="line">]).reshape(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Learning Rate alpha.</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br></pre></td></tr></table></figure>

<p>接下来我们以矩阵向量的形式定义代价函数和代价函数的梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error_function</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''Error function J definition.'''</span></span><br><span class="line">    diff = np.dot(X, theta) - y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/<span class="number">2</span>*m) * np.dot(np.transpose(diff), diff)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_function</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''Gradient of the function J definition.'''</span></span><br><span class="line">    diff = np.dot(X, theta) - y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/m) * np.dot(np.transpose(X), diff)</span><br></pre></td></tr></table></figure>

<p>最后就是算法的核心部分，梯度下降迭代计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X, y, alpha)</span>:</span></span><br><span class="line">    <span class="string">'''Perform gradient descent.'''</span></span><br><span class="line">    theta = np.array([<span class="number">1</span>, <span class="number">1</span>]).reshape(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    gradient = gradient_function(theta, X, y)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> np.all(np.absolute(gradient) &lt;= <span class="number">1e-5</span>):</span><br><span class="line">        theta = theta - alpha * gradient</span><br><span class="line">        gradient = gradient_function(theta, X, y)</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>

<p>当梯度小于1e-5时，说明已经进入了比较平滑的状态，类似于山谷的状态，这时候再继续迭代效果也不大了，所以这个时候可以退出循环！</p>
<p>完整的代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Size of the points dataset.</span></span><br><span class="line">m = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Points x-coordinate and dummy value (x0, x1).</span></span><br><span class="line">X0 = np.ones((m, <span class="number">1</span>))</span><br><span class="line">X1 = np.arange(<span class="number">1</span>, m+<span class="number">1</span>).reshape(m, <span class="number">1</span>)</span><br><span class="line">X = np.hstack((X0, X1))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Points y-coordinate</span></span><br><span class="line">y = np.array([</span><br><span class="line">    <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">11</span>, <span class="number">8</span>, <span class="number">12</span>,</span><br><span class="line">    <span class="number">11</span>, <span class="number">13</span>, <span class="number">13</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">21</span></span><br><span class="line">]).reshape(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The Learning Rate alpha.</span></span><br><span class="line">alpha = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">error_function</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''Error function J definition.'''</span></span><br><span class="line">    diff = np.dot(X, theta) - y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/<span class="number">2</span>*m) * np.dot(np.transpose(diff), diff)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_function</span><span class="params">(theta, X, y)</span>:</span></span><br><span class="line">    <span class="string">'''Gradient of the function J definition.'''</span></span><br><span class="line">    diff = np.dot(X, theta) - y</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1.</span>/m) * np.dot(np.transpose(X), diff)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span><span class="params">(X, y, alpha)</span>:</span></span><br><span class="line">    <span class="string">'''Perform gradient descent.'''</span></span><br><span class="line">    theta = np.array([<span class="number">1</span>, <span class="number">1</span>]).reshape(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    gradient = gradient_function(theta, X, y)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> np.all(np.absolute(gradient) &lt;= <span class="number">1e-5</span>):</span><br><span class="line">        theta = theta - alpha * gradient</span><br><span class="line">        gradient = gradient_function(theta, X, y)</span><br><span class="line">    <span class="keyword">return</span> theta</span><br><span class="line"></span><br><span class="line">optimal = gradient_descent(X, y, alpha)</span><br><span class="line">print(<span class="string">'optimal:'</span>, optimal)</span><br><span class="line">print(<span class="string">'error function:'</span>, error_function(optimal, X, y)[<span class="number">0</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>运行代码，计算得到的结果如下<br><img src="/images/2020/08/20200817123120.png" alt><br>所拟合出的直线如下<br><img src="/images/2020/08/20200817123134.png" alt></p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>至此，我们就基本介绍完了梯度下降法的基本思想和算法流程，并且用python实现了一个简单的梯度下降算法拟合直线的案例！<br>最后，我们回到文章开头所提出的场景假设:<br>这个下山的人实际上就代表了反向传播算法，下山的路径其实就代表着算法中一直在寻找的参数Θ，山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向，场景中观测最陡峭方向所用的工具就是微分 。在下一次观测之前的时间就是有我们算法中的学习率α所定义的。<br>可以看到场景假设和梯度下降算法很好的完成了对应</p>
<h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>梯度下降法中的”最快”过于盲目、有缺陷,所以进一步利用曲线二阶导的信息进行迭代求解，称为牛顿法</p>
<h3 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.cnblogs.com/shixiangwan/p/7532830.html" target="_blank" rel="noopener">常见的几种最优化方法（梯度下降法、牛顿法、拟牛顿法、共轭梯度法等）</a></p>
<h3 id="牛顿法作用"><a href="#牛顿法作用" class="headerlink" title="牛顿法作用"></a>牛顿法作用</h3><p>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。</p>
<h4 id="牛顿法和梯度下降法的效率对比"><a href="#牛顿法和梯度下降法的效率对比" class="headerlink" title="牛顿法和梯度下降法的效率对比"></a>牛顿法和梯度下降法的效率对比</h4><ul>
<li><p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）</p>
</li>
<li><p>根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。<br><img src="/images/2020/08/20200817124050.png" alt><br>注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。</p>
</li>
<li><p>牛顿法的优缺点总结：</p>
<ul>
<li>优点：二阶收敛，收敛速度快；</li>
<li>缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。<br><img src="/images/2020/08/20200817124343.png" alt></li>
</ul>
</li>
</ul>
</div><div class="tags"><a href="/tags/人工智能/">人工智能</a></div><div class="post-share"></div><div class="post-nav"><a href="/2019/08/18/2019-20190817-人工智能数学基础之线性代数/" class="pre">人工智能数学基础之线性代数 =&gt; 分离技术</a><a href="/2019/08/17/2019-20190817-人工智能基础说明/" class="next">人工智能的基本说明</a></div><div id="comments"><div id="lv-container" data-id="city" data-uid="MTAyMC80NDg2NC8yMTM4NQ=="></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#映射与函数"><span class="toc-text">映射与函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#极限"><span class="toc-text">极限</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#导数"><span class="toc-text">导数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#极值如何求解"><span class="toc-text">极值如何求解</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#常用函数求导公式"><span class="toc-text">常用函数求导公式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#求sigmoid函数导数"><span class="toc-text">求sigmoid函数导数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#泰勒展开式"><span class="toc-text">泰勒展开式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度下降算法"><span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-text">参考</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#原理"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#困难点"><span class="toc-text">困难点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是梯度"><span class="toc-text">什么是梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#偏微分-偏导"><span class="toc-text">偏微分(偏导)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度"><span class="toc-text">梯度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降算法的数学解释"><span class="toc-text">梯度下降算法的数学解释</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数学公式说明"><span class="toc-text">数学公式说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#α是什么含义？"><span class="toc-text">α是什么含义？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么要梯度要乘以一个负号？"><span class="toc-text">为什么要梯度要乘以一个负号？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降算法的实例"><span class="toc-text">梯度下降算法的实例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#单变量函数的梯度下降"><span class="toc-text">单变量函数的梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多变量函数的梯度下降"><span class="toc-text">多变量函数的梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度下降算法的实现"><span class="toc-text">梯度下降算法的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#简单说明最小二乘法"><span class="toc-text">简单说明最小二乘法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降算法的实现-1"><span class="toc-text">梯度下降算法的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代码实现"><span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#小结"><span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#牛顿法"><span class="toc-text">牛顿法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#参考-1"><span class="toc-text">参考</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#牛顿法作用"><span class="toc-text">牛顿法作用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#牛顿法和梯度下降法的效率对比"><span class="toc-text">牛顿法和梯度下降法的效率对比</span></a></li></ol></li></ol></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/09/24/电商数仓每个层级表的依赖关系/">电商数仓每个层级表的依赖关系</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/15/2020-20200915-Kafka工作流程分析/">Kafka工作流程分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/15/2020-20200914-JUC之Semaphore信号灯/">JUC之Semaphore信号灯</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/15/2020-20200914-JUC之CyclicBarrier循环栅栏/">JUC之CyclicBarrier循环栅栏</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/15/2020-20200914-JUC之CountDownLatch减少计数/">JUC之CountDownLatch减少计数</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/14/2020-20200914-Druid-Kylin-Presto-Impala-SparkSQL-ES比较/">Druid-Kylin-Presto-Impala-SparkSQL-ES比较</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/09/01/2020-20200901-Linux下查询进程占用的内存方法总结/">Linux下查询进程占用的内存方法总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/29/2020-20200829-项目遇到的问题之Spark/">项目遇到的问题之Spark</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/28/2020-20200828-项目遇到的问题之Sqoop/">项目遇到的问题之Sqoop</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/08/26/2020-20200826-解决Github-Page无法访问的其中一种情况/">解决Github-Page无法访问的其中一种情况</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Eclipse/">Eclipse</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Excel/">Excel</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">108</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Javascript/">Javascript</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tampermonkey/">Tampermonkey</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Ubuntu/">Ubuntu</a><span class="category-list-count">50</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/docker/">docker</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/vim/">vim</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/人工智能/">人工智能</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据结构与算法/">数据结构与算法</a><span class="category-list-count">40</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/未分类/">未分类</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/正则表达式/">正则表达式</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/视频笔记/">视频笔记</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算机操作系统/">计算机操作系统</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算机组成原理/">计算机组成原理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/设计模式/">设计模式</a><span class="category-list-count">4</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Servlet/" style="font-size: 15px;">Servlet</a> <a href="/tags/项目复盘/" style="font-size: 15px;">项目复盘</a> <a href="/tags/设计模式/" style="font-size: 15px;">设计模式</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/Junit/" style="font-size: 15px;">Junit</a> <a href="/tags/Ubuntu装机日志/" style="font-size: 15px;">Ubuntu装机日志</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/Linux命令/" style="font-size: 15px;">Linux命令</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/项目遇到的问题/" style="font-size: 15px;">项目遇到的问题</a> <a href="/tags/Jackson/" style="font-size: 15px;">Jackson</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/vim/" style="font-size: 15px;">vim</a> <a href="/tags/Eclipse/" style="font-size: 15px;">Eclipse</a> <a href="/tags/Eclipse插件/" style="font-size: 15px;">Eclipse插件</a> <a href="/tags/JUC/" style="font-size: 15px;">JUC</a> <a href="/tags/MySql/" style="font-size: 15px;">MySql</a> <a href="/tags/Spring/" style="font-size: 15px;">Spring</a> <a href="/tags/SpringMVC/" style="font-size: 15px;">SpringMVC</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/Lombok/" style="font-size: 15px;">Lombok</a> <a href="/tags/BigData/" style="font-size: 15px;">BigData</a> <a href="/tags/Tomcat/" style="font-size: 15px;">Tomcat</a> <a href="/tags/Java细节/" style="font-size: 15px;">Java细节</a> <a href="/tags/Thymeleaf/" style="font-size: 15px;">Thymeleaf</a> <a href="/tags/Linux配置/" style="font-size: 15px;">Linux配置</a> <a href="/tags/Http/" style="font-size: 15px;">Http</a> <a href="/tags/JSP-EL-JSTL/" style="font-size: 15px;">JSP/EL/JSTL</a> <a href="/tags/Struts2/" style="font-size: 15px;">Struts2</a> <a href="/tags/Hibernate/" style="font-size: 15px;">Hibernate</a> <a href="/tags/OGNL/" style="font-size: 15px;">OGNL</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/SSH/" style="font-size: 15px;">SSH</a> <a href="/tags/MyBatis/" style="font-size: 15px;">MyBatis</a> <a href="/tags/人工智能/" style="font-size: 15px;">人工智能</a> <a href="/tags/SpringBoot/" style="font-size: 15px;">SpringBoot</a> <a href="/tags/Zookeeper/" style="font-size: 15px;">Zookeeper</a> <a href="/tags/Flume/" style="font-size: 15px;">Flume</a> <a href="/tags/Jquery/" style="font-size: 15px;">Jquery</a> <a href="/tags/SpringSecurity/" style="font-size: 15px;">SpringSecurity</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/计算机组成原理/" style="font-size: 15px;">计算机组成原理</a> <a href="/tags/Excel/" style="font-size: 15px;">Excel</a> <a href="/tags/Ubuntu配置/" style="font-size: 15px;">Ubuntu配置</a> <a href="/tags/视频笔记/" style="font-size: 15px;">视频笔记</a> <a href="/tags/数据结构与算法-Java实现/" style="font-size: 15px;">数据结构与算法(Java实现)</a> <a href="/tags/Tampermonkey/" style="font-size: 15px;">Tampermonkey</a> <a href="/tags/Sqoop/" style="font-size: 15px;">Sqoop</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">97</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">93</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">36</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/">2017</a><span class="archive-list-count">37</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p><span> Copyright &copy;<a href="/." rel="nofollow">zjc.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><script>(function(d, s) {
  var j, e = d.getElementsByTagName('body')[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.appendChild(j);
})(document, 'script');
</script></body></html>