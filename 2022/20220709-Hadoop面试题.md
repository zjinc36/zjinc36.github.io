# Hadoop面试题

##  HDFS读写流程
### HDFS写流程

+   客户端问服务器我能不能上传xxx文件
+   服务器看一下这个客户端有没有上传权限,并且看一下已有文件会不会和xxx文件名冲突
+   如果如果权限验证没有问题
    *   客户端将文件切割成128m一个block,然后首先告诉NameNode要上传第一块block
    *   客户端第一个block
        -   NameNode返回三个可用的DataNode节点,D1,D2,D3给客户端(默认一个block存三份,所以返回三个DataNode)
        -   客户端收到DataNode地址后,跟其中一个节点建立pipeline,假设和D1进行通信
        -   建立成功后,D1会和D2建立pipeline,D2会和D3建立pipeline,然后逐级返回给client,这样通道就建好了
        -   从本地磁盘拿出来放到本地内存
        -   虽然我们最终上传的是128m的block,但这个过程中是以packet(64k)的形式上传
        -   D1收到pocket后会向D2发送,同时会在应答队列中增加一条等待应答的消息
        -   同理,D2会向D3发送pocket,等待应答
        -   D3收到packet后,将D3确认收到packet的应答返回给D2,D2确认应答后将D2,D3收到packet的应答返回给D1,D1则返回给客户端
        -   客户端收到应答,则一个packet上传成功
        -   其他packet同理
        -   所有packet上传成功后,则第一个block上传成功
    *   客户端上传第二个block
        -   重复第一个的流程


### HDFS读流程

+   客户端请求下载文件
+   NameNode确认客户端是否有下载权限,若有
    *   返回部分或全部block地址(一个文件block太多的话是分批返回的)
    *   返回这些block在哪个DataNode节点,其中每一个block的DataNode是有排序的,靠近客户端的排在前,心跳超时的排在后
+   客户端拿到block和每一个block所在的DataNode排序地址后,开始下载
+   下载第一个block
    *   如果客户端本身就是DataNode,直接从本地获取数据
    *   否则,和排序靠前的DataNode建立pipeline,下载数据
    *   下载完之后,进行checksum验证,如果下错下漏了,会通知NameNode,然后从下一个DataNode进行下载
+   下载第二个block
    *   重复下载第一个block的步骤
    *   下载block是并行的
+   合并block成为文件

## HDFS在读取文件的时候，如果其中一个块突然损坏了怎么办

块下载完后,会进行checksum,如果验证失败,那么会通知NameNode,然后从第二个DataNode进行下载

## HDFS在上传文件的时候，如果其中一个DataNode突然挂掉了怎么办

[在向 HDFS 中写数据的时候，当写某一副本时出错怎么处理？](https://my.oschina.net/134596/blog/1647115)
[hdfs的写流程以及namenode,datanode挂掉后处理](https://blog.csdn.net/bitbitbyte/article/details/107329688)

+   其中一个DataNode挂掉了,则客户端会收不到packet的ack消息
+   此时,关闭通道,根据ack队列,将已经发送但没有收到应答的packet重新加入packet发送队列
+   同时,客户端会向NameNode报告这件事
+   NameNode会检查有关的DataNode
    *   将不正常的下线
    *   给正常的DataNode赋予一个新的版本号,这样不正常的节点节点恢复后,由于版本号不对,会当前数据块对应的数据
    *   在正常的DataNode中,选择一个为主DataNode,其他的正常的DataNode与它通信
    *   获取所有DataNode当前数据块的数据,选择最小的一块,将每个正常的DataNode同步到该大小
+   重新建立管道
+   客户端发送剩下的数据
+   当文件关闭后,NameNode发现副本数量不对,会在其它节点上补足副本

### NameNode在启动的时候会做哪些操作

NameNode简单来说就是处理fsimage镜像文件和edit编辑文件

+   第一次启动
    *   进行格式化,创建fsimage文件
    *   启动NameNode
        -   读取fsimage文件,将内容加载进内存
        -   等待DataNode注册,发送block report
    *   启动DataNode
        -   向NameNode注册
        -   发送block report
        -   检查fsimage中记录的block数量和发送的block report的数量是否相同
    *   对文件系统进行操作
        -   文件系统的元数据信息会先写入fsimage,然后会将这些元数据信息修改定期写入edit文件
+   第二次启动
    *   读取fsimage和edit文件
    *   将fsimage和edit合并成新的fsimage文件
    *   创建新的edit文件
    *   启动DataNode

### Secondary NameNode了解吗，它的工作机制是怎样的

+   2NN询问NN是否需要checkpoint
    *   checkpoint触发条件
        -   edit中的数据满了
        -   定时时间到了
+   NN中,创建一个新的edit空文件
+   2NN从NN中拷贝两个文件
    *   旧的edit文件
    *   fsimage文件
+   在2NN中,将这两个文件读到内存中,然后进行合并,合并完成之后得到一个新的fsimage文件,名叫fsimage.checkpoint
+   将这个新的fsimage.checkpoint文件拷贝回NN,然后重命名为fsimage

由上述过程可以看出,如果NN的元数据丢失,是可以从2NN中恢复一部分数据的,但是还未合并的edit文件数据就会完全丢失

### Secondary NameNode不能恢复NameNode的全部数据，那如何保证NameNode数据存储安全

+   HA机制
    *   有两个NameNode,假定为NN1和NN2,其中NN1为主节点,NN2为备份节点
+   如何同步两个NN节点
    *   有一个共享存储
    *   NN1的元数据改变时,会将改变写入本地edit,同时也写一份到共享存储的edit,只有写入共享存储才能认定写入edit成功
    *   NN2定期从共享存储获取edit文件,以便进行主备切换
+   主NN挂了,如何切换
    *   首先zookeeper中存放这两台NN的状态
    *   其次zookeeper中有zkfc(zookeeperFailoverController)监控两台NN的状态
    *   如果standby节点的zkfc发现主节点挂了,那么就会发送隔离(fencing)命令给原本的Active NameNode,之后将standby节点设为active
        -   在进行fencing时,会执行以下的操作
        -   首先尝试调用这个旧的Active NameNode的HAServiceProtocol RPC接口的transitionToStandby方法,尝试将NN转为Standby状态
        -   如果失败,会执行Hadoop中预定义的隔离措施
            +   sshfence:通过ssh登录到目标机器上,执行命令 fuser 将对应的进程杀死
            +   shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离
    
### 在NameNode HA中，会出现脑裂问题吗？怎么解决脑裂

分情况讨论

+   主NN挂了
    *   主NN挂了,不会出现脑裂
+   监控主NN的zkfc进程挂了
    *   在zookeeper中,根据存的面包屑进行fencing
+   主NN的网断了
    *   不会脑裂,但更严重,集群会挂
    *   因为为了防止脑裂,Standby节点变为Active之前,要先杀死旧的Active NameNode
    *   但是由于网断了,则永远无法杀死旧的Active,那么Standby就无法变为Active,所以集群会挂
    
### 小文件过多会有什么危害，如何避免

+   小文件过多,则对应的元数据也会很多,要很多内存(每个大概150byte),也会造成寻址太慢
+   如何解决小文件过多
    *   根本方法,就是不要产生小文件
    *   将多个小文件压缩成一个文件
        -   在hadoop中可以将多个小文件压缩成一个har文件
        -   这个压缩在逻辑上我们仍然能透明的使用这些小文件,但对于存储来讲,这些小文件只有一个元数据
        

    

    
